{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "410e16c9-5a81-49fd-8d6d-be98bbd6f922",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bibliotheken importieren\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import spacy\n",
    "import re\n",
    "import os\n",
    "from symspellpy import SymSpell, Verbosity\n",
    "from importlib.resources import files, as_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80af690a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verzeichnisse für Modelle und Plots erstellen, falls sie nicht existieren\n",
    "os.makedirs('../models', exist_ok=True)\n",
    "os.makedirs('../results', exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5d204a6-76c8-4d81-9ede-141150c4161e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download für englischensprachigen Datensatz\n",
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d412f54-8a51-4e54-8730-ef69786d7ab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialisierungen & Downloads\n",
    "try:\n",
    "    nltk.download('punkt')\n",
    "    nltk.download('stopwords')\n",
    "    nltk.download('wordnet')\n",
    "    nlp = spacy.load('en_core_web_sm')\n",
    "    print(\"NLP-Bibliotheken erfolgreich initialisiert.\")\n",
    "except Exception as e:\n",
    "    print(\"Fehler beim Laden der NLP-Komponenten:\")\n",
    "    print(e)\n",
    "\n",
    "# Spacy-Modell laden\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Prüfen ob der Datensatz vorhanden ist und laden\n",
    "try: \n",
    "    # Pfad ggf. anpassen\n",
    "    data = pd.read_csv('../data/consumer_complaints.csv', low_memory=False)\n",
    "    print(f\"Datensatz vorhanden und geladen: {data.shape[0]} Zeilen und {data.shape[1]} Spalten\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Datei nicht gefunden. Bitte lade den Datensatz herunter und speichere ihn im 'data'-Ordner.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9efe4ab-ff31-41f5-aee6-96ce41bb4f24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SymSpell initialisieren, https://github.com/wolfgarbe/SymSpell\n",
    "sym_spell = SymSpell(max_dictionary_edit_distance=2, prefix_length=7)\n",
    "\n",
    "# Lade die Ressource aus dem symspellpy-Paket\n",
    "# Pfad zur Wörterbuch-Datei holen\n",
    "with as_file(files(\"symspellpy\") / \"frequency_dictionary_en_82_765.txt\") as dictionary_path:\n",
    "    sym_spell.load_dictionary(str(dictionary_path), term_index=0, count_index=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b814f03e-a9cb-4be2-b934-4c128be5689d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zeigt die ersten 5 Zeilen des DataFrames an (Vorschau auf den Datensatz)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb064ef0-4ade-4020-8bfd-8c1ba60ea74f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zeigt die Struktur des DataFrames: Spaltennamen, Datentypen und Anzahl der Nicht-Null-Werte\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2d2bf2d-8ed6-4ad2-9cc2-14038335ae32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zeigt statistische Kennzahlen für numerische Spalten (z. B. Mittelwert, Standardabweichung, Min/Max)\n",
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "247d0566-0578-466e-b802-9b6176fa4d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zählt die Anzahl der fehlenden (NaN) Werte in jeder Spalte\n",
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8b5d5a3-e1bf-4948-8959-6c0a5e766be6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prüft, ob die Spalte 'consumer_complaint_narrative' im DataFrame existiert\n",
    "if 'consumer_complaint_narrative' in data.columns:\n",
    "    # Filtert nur die Zeilen, in denen ein Beschwerdetext vorhanden ist (ohne NaN)\n",
    "    complaints = data[data['consumer_complaint_narrative'].notna()]['consumer_complaint_narrative']\n",
    "    # Gibt die Anzahl der Beschwerden mit Text aus\n",
    "    print(f\"Anzahl der Beschwerden mit Text: {len(complaints):,} von {len(data):,}\".replace(\",\",\".\"))\n",
    "else:\n",
    "    print(\"Spalte mit Beschwerdetexten nicht gefunden. Überprüfe die Spaltennamen:\")\n",
    "    print(data.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa40ab5b-3dc7-4bcc-9e7a-95fd4921c196",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funktion zur Textvorverarbeitung\n",
    "def preprocess_text(text):\n",
    "    # Prüfen ob text ein string ist\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "\n",
    "    # 1. Kleinbuchstaben\n",
    "    text = text.lower()\n",
    "\n",
    "    # 2. Entfernen von Sonderzeichen und Zahlen (nur Buchstaben und Leerzeichen behalten)\n",
    "    text = re.sub(r'[^a-z\\s]', '', text) # Nur Kleinbuchstaben und Leerzeichen beibehalten, wegen ^ nicht diese Zeichen entfernen !!!\n",
    "\n",
    "    # 3. Tokenisierung (Text in Wörter zerlegen)\n",
    "    tokens = word_tokenize(text, preserve_line=True)\n",
    "\n",
    "    # 4. Stoppwörter entfernen (häufige Wörter wie 'the', 'is', 'in')\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "\n",
    "    # 5. Lemmatisierung (Wörter auf ihre Grundform zurückführen, z.B. 'running' -> 'run')\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "\n",
    "    # 5.5 Entfernen von Mustern wie 'xx', 'xxx', 'xxxx' usw. | Dieser Schritt ist erst nach der Erstellung der ersten Tabelle mit TF-IDF aufgefallen.\n",
    "    # re.fullmatch prüft das ganze wort auf das Muster\n",
    "    tokens = [word for word in tokens if not re.fullmatch(r'x{2,}', word)]\n",
    "\n",
    "    # 5.6 : Entfernen von Wörtern, die x-Sequenzen enthalten (z.B. \"xxxx1234\" oder \"xxxxbank\")\n",
    "    tokens = [word for word in tokens if not re.search(r'x{2,}', word)]\n",
    "\n",
    "    # 6. Entfernen von sehr kurzen Wörtern (oft Rauschen)\n",
    "    tokens = [word for word in tokens if len(word) > 2]\n",
    "\n",
    "    # 7. Rechtschreibkorrektur mit SymSpellPy für allgemeines Englisch\n",
    "    corrected_tokens = []\n",
    "    for word in tokens:\n",
    "        # Nur für Wörter mit mindestens 3 Buchstaben die Rechtschreibkorrektur anwenden\n",
    "        if len(word) <= 2:\n",
    "            corrected_tokens.append(word)\n",
    "            continue\n",
    "            \n",
    "        # SymSpell für allgemeine Rechtschreibkorrektur verwenden\n",
    "        # https://symspellpy.readthedocs.io/en/latest/api/symspellpy.html\n",
    "        try:\n",
    "            suggestions = sym_spell.lookup(word, Verbosity.CLOSEST, max_edit_distance=2)\n",
    "            if suggestions:\n",
    "                # Wähle die wahrscheinlichste Korrektur, wenn das Konfidenzmaß hoch genug ist\n",
    "                if suggestions[0].distance <= 1:  # Nur Korrekturen mit geringer Edit-Distanz\n",
    "                    corrected_tokens.append(suggestions[0].term)\n",
    "                else:\n",
    "                    corrected_tokens.append(word)  # Original behalten wenn unsicher\n",
    "            else:\n",
    "                corrected_tokens.append(word)\n",
    "        except:\n",
    "            corrected_tokens.append(word)\n",
    "\n",
    "    # 8. Entfernen von sehr kurzen Wörtern (oft Rauschen)\n",
    "    corrected_tokens = [word for word in corrected_tokens if len(word) > 2]\n",
    "\n",
    "    # Rückgabe als einzelner String mit Leerzeichen getrennt\n",
    "    return ' '.join(corrected_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a409eb97-3210-412f-99d0-8c94648f743d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vorverarbeitung auf einer kleinen Stichprobe testen\n",
    "print(\"\\nTest der Vorverarbeitung an Beispielen:\")\n",
    "sample = complaints.sample(5)\n",
    "for i, text in enumerate(sample):\n",
    "    print(f\"Original {i+1}: {text[:100]}...\")\n",
    "    print(f\"Verarbeitet {i+1}: {preprocess_text(text)[:100]}...\")\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9407a9bc-af0f-46db-ab05-b7c098053212",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vorverarbeitung auf alle Texte anwenden (kann etwas dauern | bei mir ca. 3 Minuten))\n",
    "# Hinweis: Bei sehr großen Datensätzen kann dies länger dauern.\n",
    "processed_complaints = complaints.apply(preprocess_text)\n",
    "\n",
    "# Speichern für die nächste Phase\n",
    "processed_df = pd.DataFrame({'processed_text': processed_complaints})\n",
    "processed_df.to_csv('../data/processed_complaints_v2.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_projekt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
