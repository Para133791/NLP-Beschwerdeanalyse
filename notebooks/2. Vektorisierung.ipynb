{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d03fca32-aa2b-405f-b22d-9d24efb5184e",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "Hier werden die vorverarbeiteten Beschwerde-Texte in numerische Vektoren umgewandelt.\n",
    "Dazu nutzen wir:\n",
    "# 1. TF-IDF: Gewichtung relevanter Begriffe basierend auf Häufigkeit und Seltenheit\n",
    "# 2. Word2Vec: Erzeugung semantisch reicher Worteinbettungen\n",
    "\n",
    "Die Ergebnisse werden wieder gespeichert um mit dem nächsten Notebook die Themen zu Extrahieren und Analysieren\n",
    "\n",
    "'''\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "from nltk.tokenize import word_tokenize\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "import joblib  # Sichere Alternative zu pickle für das Speichern von Modellen\n",
    "import json    # Zum Speichern von Metadaten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dd1a4a22-7327-4c92-bcda-6cc203ec152b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datei gefunden.\n",
      "Geladene Texte: 66806\n"
     ]
    }
   ],
   "source": [
    "# Versuchen Daten einzulesen\n",
    "try:\n",
    "    processed_df = pd.read_csv('../data/processed_complaints_v2.csv')\n",
    "    print(\"Datei gefunden.\")\n",
    "    print(f\"Geladene Texte: {len(processed_df)}\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Fehler: Datei 'processed_complaints_v2.csv' nicht gefunden. Zuerst 1. Datenvorverarbeitung ausführen\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fe76afbc-78d6-4f17-ad73-fe42474043b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Korpus erstellen, NaN-Werte entfernen sowie Numpy Array zu Listen konvertieren\n",
    "corpus = processed_df['processed_text'].dropna().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3358628f-a1b5-46f5-a200-371cd5c5fad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################################################\n",
    "# 1. TF-IDF Vektorisierung\n",
    "##########################################################\n",
    "tfidf_vectorizer = TfidfVectorizer(\n",
    "    max_features = 2000, # die 2000 Häufigsten Themen | Vielleicht höher stellen oder niedriger stellen\n",
    "    min_df = 5,          # Begriffe müssen in mindestens 5 Dokumenten vorkommen\n",
    "    max_df= 0.75,        # Begriffe, die mehr als 85% der Dokumente vorkommen\n",
    "    ngram_range=(1, 2)   # Berücksichtigt Unigramme und Bigramme (not good statt good)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dcbe8f3f-0074-4111-b071-02b99f3d9503",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF-Matrix erstellt mit Shape: (66804, 2000)\n",
      "Beispielbegriffe: ['ability', 'able', 'able get', 'able pay', 'absolutely', 'abuse', 'abusive', 'accept', 'acceptance', 'accepted']\n"
     ]
    }
   ],
   "source": [
    "# Vektorisierung durchführen\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(corpus)\n",
    "print(f\"TF-IDF-Matrix erstellt mit Shape: {tfidf_matrix.shape}\")\n",
    "print(\"Beispielbegriffe:\", list(tfidf_vectorizer.get_feature_names_out()[:10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3d50fe87",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../models/tfidf_vectorizer.joblib'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Speichern des TF-IDF Modells\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m joblib\u001b[38;5;241m.\u001b[39mdump(tfidf_vectorizer, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../models/tfidf_vectorizer.joblib\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      3\u001b[0m np\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../models/tfidf_matrix.npy\u001b[39m\u001b[38;5;124m'\u001b[39m, tfidf_matrix\u001b[38;5;241m.\u001b[39mtoarray())\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Hier kam es zu einem Fehler mit einem Datentyp int64 um das Vokabular als JSON zu speichern. \u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Deshalb wird der Datentyp hier noch geändert. \u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\nlp_projekt\\Lib\\site-packages\\joblib\\numpy_pickle.py:552\u001b[0m, in \u001b[0;36mdump\u001b[1;34m(value, filename, compress, protocol, cache_size)\u001b[0m\n\u001b[0;32m    550\u001b[0m         NumpyPickler(f, protocol\u001b[38;5;241m=\u001b[39mprotocol)\u001b[38;5;241m.\u001b[39mdump(value)\n\u001b[0;32m    551\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m is_filename:\n\u001b[1;32m--> 552\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(filename, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m    553\u001b[0m         NumpyPickler(f, protocol\u001b[38;5;241m=\u001b[39mprotocol)\u001b[38;5;241m.\u001b[39mdump(value)\n\u001b[0;32m    554\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../models/tfidf_vectorizer.joblib'"
     ]
    }
   ],
   "source": [
    "# Speichern des TF-IDF Modells\n",
    "joblib.dump(tfidf_vectorizer, '../models/tfidf_vectorizer.joblib')\n",
    "np.save('../models/tfidf_matrix.npy', tfidf_matrix.toarray())\n",
    "\n",
    "# Hier kam es zu einem Fehler mit einem Datentyp int64 um das Vokabular als JSON zu speichern. \n",
    "# Deshalb wird der Datentyp hier noch geändert. \n",
    "\n",
    "vocabulary_clean = {k: int(v) for k, v in tfidf_vectorizer.vocabulary_.items()}\n",
    "\n",
    "# Speichern des Vokabulars als JSON für bessere Interpretierbarkeit\n",
    "vocabulary = tfidf_vectorizer.vocabulary_\n",
    "with open('../models/tfidf_vocabulary.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(vocabulary_clean, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5e4826c-ab6a-400e-959b-60df4bb4500c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mittelwert je Spalte (= Begriff) berechnen\n",
    "tfidf_means = np.asarray(tfidf_matrix.mean(axis=0)).flatten()\n",
    "\n",
    "# Begriffe holen\n",
    "terms = tfidf_vectorizer.get_feature_names_out()\n",
    "\n",
    "# Top 20 Begriffe mit höchstem durchschnittlichen TF-IDF-Wert\n",
    "top_n = 20\n",
    "top_indices = tfidf_means.argsort()[::-1][:top_n]\n",
    "top_terms = [terms[i] for i in top_indices]\n",
    "top_scores = [tfidf_means[i] for i in top_indices]\n",
    "\n",
    "# Balkendiagramm erstellen\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.barh(top_terms[::-1], top_scores[::-1])  # Ohne [::-1] sind die niedrigsten Werte oben\n",
    "plt.xlabel(\"Durchschnittlicher TF-IDF-Wert\")\n",
    "plt.title(\"Top 20 Begriffe nach durchschnittlichem TF-IDF\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b498f29a",
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################################################\n",
    "# 2. Bag-of-Words Vektorisierung für LDA\n",
    "##########################################################\n",
    "\n",
    "# CountVectorizer für Bag-of-Words erstellen\n",
    "count_vectorizer = CountVectorizer(\n",
    "    max_features=5000,\n",
    "    min_df=5,\n",
    "    max_df=0.75,\n",
    "    ngram_range=(1, 2)\n",
    ")\n",
    "\n",
    "# Count Matrix berechnen\n",
    "count_matrix = count_vectorizer.fit_transform(corpus)\n",
    "print(f\"Count Matrix erstellt: {count_matrix.shape[0]} Dokumente, {count_matrix.shape[1]} Features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc4e1b72-8045-43d6-8d74-1caf59ce3267",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
