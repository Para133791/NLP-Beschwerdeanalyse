{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8857d41e-0443-47ab-8831-622267081e02",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "import matplotlib.cm as cm\n",
    "import seaborn as sns\n",
    "import json\n",
    "import joblib\n",
    "from sklearn.decomposition import TruncatedSVD, LatentDirichletAllocation\n",
    "from wordcloud import WordCloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6238dfb-95f2-4398-b2aa-b41a0d712ed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stile für Visualisierungen \n",
    "plt.style.use('seaborn-v0_8-whitegrid')  # Matplotlib-Stil mit weißem Hintergrund und Gitternetz\n",
    "plt.rcParams['figure.figsize'] = (12, 8) # Standardgröße der Plots: 12x8 Zoll\n",
    "plt.rcParams['font.size'] = 12           # Standard-Schriftgröße: 12pt\n",
    "sns.set_style(\"whitegrid\")               # Seaborn-Stil: weißer Hintergrund mit Gitternetz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ddce46e-c3df-4274-8178-c17d9096cb95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Laden der vorverarbeiteten Daten aus den vorherigen Notebooks\n",
    "try:\n",
    "    # TF-IDF Matrix für LSA\n",
    "    tfidf_matrix = np.load('../models/tfidf_matrix.npy')\n",
    "    tfidf_vectorizer = joblib.load('../models/tfidf_vectorizer.joblib')\n",
    "    \n",
    "    # Count Matrix für LDA \n",
    "    count_matrix = np.load('../models/count_matrix_for_lda.npy')\n",
    "    count_vectorizer = joblib.load('../models/count_vectorizer_for_lda.joblib')\n",
    "    \n",
    "    print(\"Alle Vektordaten erfolgreich geladen!\")\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"Fehler beim Laden der vorverarbeiteten Daten: {e}\")\n",
    "    print(\"Erst 1. Datenvorverarbeitung vollständig ausführen und danach 2. Vektorisierung ausführen, damit alle Datensätze vorhanden sind.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86f8929b-7e58-4720-a71d-b3485a861c97",
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################################################\n",
    "# 1. LSA mit TF-IDF\n",
    "##########################################################\n",
    "\n",
    "# Anzahl der Themen für LSA auswählen\n",
    "n_components_lsa = 10 # Variable kann geändert werden um verschiedene Ausgaben zu testen, 10 erstmal für eine bessere Übersicht gewählt.\n",
    "\n",
    "# LSA-Modell erstellen und trainieren\n",
    "print('LSA-Modell wird trainiert...')\n",
    "lsa_model = TruncatedSVD(n_components=n_components_lsa, random_state=42)\n",
    "lsa_components = lsa_model.fit_transform(tfidf_matrix)\n",
    "print('LSA-Modell fertig trainiert')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3880748d-7b48-405d-b835-3046c3877290",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Erklärte Varianz der Komponenten\n",
    "explained_variance = lsa_model.explained_variance_ratio_.sum()\n",
    "print(f\"Erklärte Varianz durch {n_components_lsa} Komponenten: {explained_variance:.2%}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbb2a65d-be67-4f26-a66f-40b83d1ec67b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisierung der erklärten Varianz\n",
    "\n",
    "plt.plot(range(1, n_components_lsa + 1), lsa_model.explained_variance_ratio_, 'b-', marker='o')\n",
    "plt.bar(range(1, n_components_lsa + 1), lsa_model.explained_variance_ratio_)\n",
    "plt.xlabel('Komponenten')\n",
    "plt.ylabel('Erklärte Varianz')\n",
    "plt.title('Erklärte Varianz pro LSA-Komponente')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4baa6c5b-47a0-4389-84b2-c2b4a62efa93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top Begriffe je LSA-Komponente identifizieren\n",
    "feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "lsa_components_df = pd.DataFrame()\n",
    "\n",
    "# Speichern der wichtigsten Begriffe für jede Komponente\n",
    "n_top_words = 20\n",
    "lsa_top_words = {}\n",
    "\n",
    "for i, component in enumerate(lsa_model.components_): # Iterate über jedes LSA-Thema\n",
    "    # Indizes der Wörter mit den höchsten Werten\n",
    "    top_indices = component.argsort()[:-n_top_words-1:-1] \n",
    "    top_words = [feature_names[idx] for idx in top_indices]\n",
    "    top_weights = [component[idx] for idx in top_indices]\n",
    "    \n",
    "    # Als Dataframe speichern\n",
    "    component_df = pd.DataFrame({\n",
    "        'Word': top_words,\n",
    "        'Weight': top_weights\n",
    "    })\n",
    "    lsa_components_df = pd.concat([lsa_components_df, component_df], axis=1)\n",
    "    \n",
    "    # Für JSON speichern\n",
    "    lsa_top_words[f\"topic_{i+1}\"] = [\n",
    "        {\"word\": word, \"weight\": float(weight)} \n",
    "        for word, weight in zip(top_words, top_weights)\n",
    "    ]\n",
    "    \n",
    "    print(f\"LSA Thema {i+1}: {', '.join(top_words[:10])}\")\n",
    "\n",
    "\n",
    "# LSA-Themen als JSON speichern\n",
    "with open('../results/lsa_topics.json', 'w') as f:\n",
    "    json.dump(lsa_top_words, f, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93294435-0c38-4bd2-ae7c-78bba714220b",
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################################################\n",
    "# 2. Latent Drichilet Allocation (LDA) mit Bag Of Words\n",
    "##########################################################\n",
    "\n",
    "# Funktion zur Berechnung von Kohärenzwerten (hier als Log-Likelihood) und Perplexity für verschiedene Topic-Anzahlen\n",
    "def compute_coherence_values(count_matrix, count_vect, start=2, limit=20, step=2):\n",
    "    coherence_values = []    # Liste zur Speicherung der Kohärenzwerte (Log-Likelihoods) für jedes Modell\n",
    "    model_list = []          # Liste zur Speicherung der trainierten LDA-Modelle\n",
    "    perplexity_values = []   # Liste zur Speicherung der Perplexity-Werte (ein Maß für Modellqualität)\n",
    "\n",
    "    # Schleife über die gewünschte Anzahl an Themen (z. B. 2, 4, 6, ..., 20)\n",
    "    for num_topics in range(start, limit+1, step):\n",
    "        print(f\"LDA Modell mit {num_topics} Themen trainieren...\")\n",
    "\n",
    "        # https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.LatentDirichletAllocation.html\n",
    "        # Initialisierung eines LDA-Modells mit aktueller Themenanzahl\n",
    "        lda = LatentDirichletAllocation(\n",
    "            n_components=num_topics,       # Anzahl der zu entdeckenden Themen (Topics)\n",
    "            max_iter=5,                    # Anzahl der Trainingsdurchläufe (Iterationen)\n",
    "            learning_method='online',      # Lernmethode: 'online' für inkrementelles Training (schneller bei großen Datensätzen)\n",
    "            random_state=42,               # Zufallsstart fixieren für Reproduzierbarkeit\n",
    "            batch_size=128,                # Anzahl der Dokumente pro Mini-Batch beim Training\n",
    "            n_jobs=-1,                     # Alle verfügbaren CPU-Kerne nutzen\n",
    "            verbose=0,                     # Keine Ausgaben während des Trainings (0 = keine Ausgabe, 1 = Fortschritt anzeigen)\n",
    "        )\n",
    "\n",
    "        # LDA-Modell auf der Dokument-Term-Matrix trainieren\n",
    "        lda.fit(count_matrix)\n",
    "\n",
    "        # Perplexity berechnen (niedriger ist besser; misst, wie gut das Modell neue Daten erklärt)\n",
    "        perplexity_values.append(lda.perplexity(count_matrix))\n",
    "\n",
    "        # Das trainierte Modell zur späteren Analyse speichern\n",
    "        model_list.append(lda)\n",
    "\n",
    "        # Log-Likelihood berechnen (höher ist besser; Maß für die Modellgüte während des Trainings)\n",
    "        coherence_values.append(lda.score(count_matrix))\n",
    "\n",
    "    # Rückgabe: Liste der Modelle, Liste der Kohärenzwerte, Liste der Perplexity-Werte\n",
    "    return model_list, coherence_values, perplexity_values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df4d990e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Die Variablen start, limit und step definieren die Anzahl der Themen, die getestet werden sollen und sind für die Funktion compute_coherence_values() unter dieser Zelle.\n",
    "# Das ist extra hier, damit nach dem Training der Modelle, die Darstellung ohne erneutiges Training der Modelle erfolgen kann.\n",
    "# Beispiel: range(2, 21, 2) → [2, 4, 6, ..., 20]\n",
    "start, limit, step = 2, 10, 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e26374d5-e3fe-41c6-8c7c-c566ef5a5241",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nach der ersten Darstellung der LDA-Modelle mit 20 Themen, wurde die Anzahl der Themen auf 10 reduziert, weil nach 10 Themen nicht die Perplexity nicht mehr so stark fällt. \n",
    "# Dadurch war das Training nach 6 Minuten (auf dem Macbook Pro) und 11 Minuten (am anderen Computer) fertig.\n",
    "# Diese Funktion nimmt einige Zeit in Anspruch, deshalb wurde auch n_jobs auf alle CPUs gestellt. \n",
    "# Die ersten Test: Mit 5 Durchläufen ca. 20 Minuten.  Bei 10 waren es mehr als 1h. Durch max_iter auf 5, bei 10 Durchgängen ca. 28 Minuten.\n",
    "# Definiert den Bereich für die Anzahl der LDA-Themen:\n",
    "# start = kleinste Themenanzahl, limit = größte Themenanzahl, step = Schrittweite zwischen den Tests -> für range()\n",
    "\n",
    "model_list, coherence_values, perplexity_values = compute_coherence_values(\n",
    "    count_matrix, count_vectorizer, start=start, limit=limit, step=step\n",
    ")\n",
    "\n",
    "print(\"Training der LDA-Modelle abgeschlossen.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc402c4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finde den Index des besten Wertes (höchste Kohärenz / Log Likelihood) in der Liste\n",
    "optimal_model_idx = coherence_values.index(max(coherence_values))\n",
    "\n",
    "# Rechne aus, wie viele Themen das Modell mit dem besten Wert hatte\n",
    "optimal_num_topics = start + optimal_model_idx * step\n",
    "\n",
    "# (Startwert + Index * Schrittweite)\n",
    "print(f\"Optimale Anzahl Themen basierend auf Log Likelihood: {optimal_num_topics}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94e1c245",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kohärenzwerte visualisieren\n",
    "x = range(start, limit+1, step)\n",
    "\n",
    "\n",
    "# Plot für Perplexity/Verwirrung/Verworrenheit (niedriger ist besser)\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(x, perplexity_values, marker='o')\n",
    "plt.xlabel(\"Anzahl Themen\")\n",
    "plt.ylabel(\"Perplexity\")\n",
    "plt.title(\"Perplexity vs. Themenanzahl\")\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Bestes LDA-Modell verwenden\n",
    "best_lda_model = model_list[optimal_model_idx]\n",
    "\n",
    "# LDA-Themen Visualisierung\n",
    "feature_names = count_vectorizer.get_feature_names_out()\n",
    "lda_top_words = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "745d9128",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteriere über jedes LDA-Thema und finde die Top-Wörter\n",
    "for i, topic in enumerate(best_lda_model.components_):  # Iteration über jedes LDA-Thema\n",
    "    # Indizes der Wörter mit den höchsten Werten | Der Slice-Operator [:-n_top_words-1:-1] gibt die n_top_words größten Werte zurück\n",
    "    # und sortiert sie in absteigender Reihenfolge\n",
    "    top_indices = topic.argsort()[:-n_top_words-1:-1]\n",
    "    top_words = [feature_names[j] for j in top_indices] # Liste der Wörter mit den höchsten Werten\n",
    "    top_weights = [topic[j] for j in top_indices]       # Liste der Gewichtungen der Wörter\n",
    "    \n",
    "    lda_top_words[f\"topic_{i+1}\"] = [                   # \n",
    "        {\"word\": word, \"weight\": float(weight)}         # Speichern der Wörter und deren Gewichtung\n",
    "        for word, weight in zip(top_words, top_weights) # Liste der Wörter und deren Gewichtung\n",
    "    ]\n",
    "    \n",
    "    print(f\"LDA Thema {i+1}: {', '.join(top_words[:10])}\") # Ausgabe der ersten 10 Begriffe, kann geändert werden "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4e4da23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LDA-Themen als JSON speichern\n",
    "with open('../results/lda_topics.json', 'w') as f:\n",
    "    json.dump(lda_top_words, f, indent=4)\n",
    "\n",
    "# LDA-Modell speichern\n",
    "joblib.dump(best_lda_model, '../models/best_lda_model.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3b36665",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 10))\n",
    "for topic_idx, topic in enumerate(best_lda_model.components_):                  # Iteriere über alle Themen im besten LDA-Modell\n",
    "    top_features_ind = topic.argsort()[:-10 - 1:-1]                             # Indexe der Top 10 Wörter im Thema nach Gewichtung (absteigend)\n",
    "    top_features = [feature_names[i] for i in top_features_ind]                 # Hole die tatsächlichen Wörter (Feature-Namen) zu den Indexen\n",
    "    \n",
    "    # Gewichtungen in Prozent umrechnen (sieht besser aus, als riesige Zahlen)\n",
    "    # topic[top_features_ind] gibt die Gewichtungen der Top-Wörter zurück, und topic.sum() gibt die Summe aller Gewichtungen im Thema zurück \n",
    "    weights = topic[top_features_ind] / topic.sum() * 100\n",
    "\n",
    "    # Barplot erstellen\n",
    "    # plt.subplot(int(np.ceil(optimal_num_topics / 2)), 2, topic_idx + 1) # Anzahl der Subplots in der Figur (2 Spalten, so viele Zeilen wie nötig)\n",
    "    ax = plt.subplot(int(np.ceil(optimal_num_topics / 2)), 2, topic_idx + 1)\n",
    "    \n",
    "    # plt.barh(top_features, weights) # Erstelle ein horizontales Balkendiagramm mit den Top-Wörtern und deren Gewichtungen\n",
    "    ax.barh(top_features, weights)\n",
    "    ax.set_title(f'Topic {topic_idx + 1}')\n",
    "    ax.invert_yaxis()\n",
    "    ax.tick_params(axis='both', which='major', labelsize=8)\n",
    "    ax.set_xlabel('Wortgewichtung im Thema (%)', fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cad432a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Funktion zur Erstellung von Wortwolken für die Themen\n",
    "def create_wordcloud_for_topics(model, feature_names, n_topics, topic_type=\"LDA\"):\n",
    "    # Farben für jedes Thema definieren\n",
    "    # Hier wird eine Farbkarte mit 10 Farben erstellt, die für die Themen verwendet wird\n",
    "    colors = cm.tab10(np.linspace(0, 1, n_topics))\n",
    "    \n",
    "    # Wortwolken für jedes Thema erstellen \n",
    "    fig, axes = plt.subplots(int(np.ceil(n_topics/2)), 2, figsize=(16, int(np.ceil(n_topics/2)*8))) # \n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    # Iteriere über jedes Thema und erstelle eine Wortwolke\n",
    "    for i, topic in enumerate(model.components_):\n",
    "        top_indices = topic.argsort()[:-50-1:-1] # \n",
    "        top_words = {feature_names[j]: float(topic[j]) for j in top_indices}\n",
    "        \n",
    "        # Wortwolke mit gewichteten Wörtern erstellen\n",
    "        wordcloud = WordCloud(\n",
    "            background_color='white',\n",
    "            width=800, \n",
    "            height=400,\n",
    "            max_words=35, # maximale Anzahl der Wörter in der Wolke, darf gerne geändert werden. \n",
    "\n",
    "            \n",
    "            colormap=f'tab10',\n",
    "            color_func=lambda *args, **kwargs: tuple(int(v*255) for v in colors[i][:3]), #\n",
    "            prefer_horizontal=1.0\n",
    "        ).generate_from_frequencies(top_words)\n",
    "        \n",
    "        axes[i].imshow(wordcloud, interpolation='bilinear')\n",
    "        axes[i].set_title(f\"{topic_type} Thema {i+1}\", fontsize=16)\n",
    "        axes[i].axis('off')\n",
    "    \n",
    "    # Leere Subplots ausblenden\n",
    "    for i in range(n_topics, len(axes)):\n",
    "        axes[i].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'../results/{topic_type.lower()}_wordclouds.png', dpi=300, bbox_inches='tight')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31414b5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word Clouds für LDA-Themen\n",
    "create_wordcloud_for_topics(\n",
    "    best_lda_model, \n",
    "    count_vectorizer.get_feature_names_out(), \n",
    "    optimal_num_topics, \n",
    "    \"LDA\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6215293",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word Clouds für LSA-Themen\n",
    "create_wordcloud_for_topics(\n",
    "    lsa_model, \n",
    "    tfidf_vectorizer.get_feature_names_out(), \n",
    "    n_components_lsa, \n",
    "    \"LSA\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_projekt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
